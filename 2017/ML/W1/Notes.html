<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Notes</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style type="text/css">
  @font-face {
    font-family: octicons-link;
    src: url(data:font/woff;charset=utf-8;base64,d09GRgABAAAAAAZwABAAAAAACFQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABEU0lHAAAGaAAAAAgAAAAIAAAAAUdTVUIAAAZcAAAACgAAAAoAAQAAT1MvMgAAAyQAAABJAAAAYFYEU3RjbWFwAAADcAAAAEUAAACAAJThvmN2dCAAAATkAAAABAAAAAQAAAAAZnBnbQAAA7gAAACyAAABCUM+8IhnYXNwAAAGTAAAABAAAAAQABoAI2dseWYAAAFsAAABPAAAAZwcEq9taGVhZAAAAsgAAAA0AAAANgh4a91oaGVhAAADCAAAABoAAAAkCA8DRGhtdHgAAAL8AAAADAAAAAwGAACfbG9jYQAAAsAAAAAIAAAACABiATBtYXhwAAACqAAAABgAAAAgAA8ASm5hbWUAAAToAAABQgAAAlXu73sOcG9zdAAABiwAAAAeAAAAME3QpOBwcmVwAAAEbAAAAHYAAAB/aFGpk3jaTY6xa8JAGMW/O62BDi0tJLYQincXEypYIiGJjSgHniQ6umTsUEyLm5BV6NDBP8Tpts6F0v+k/0an2i+itHDw3v2+9+DBKTzsJNnWJNTgHEy4BgG3EMI9DCEDOGEXzDADU5hBKMIgNPZqoD3SilVaXZCER3/I7AtxEJLtzzuZfI+VVkprxTlXShWKb3TBecG11rwoNlmmn1P2WYcJczl32etSpKnziC7lQyWe1smVPy/Lt7Kc+0vWY/gAgIIEqAN9we0pwKXreiMasxvabDQMM4riO+qxM2ogwDGOZTXxwxDiycQIcoYFBLj5K3EIaSctAq2kTYiw+ymhce7vwM9jSqO8JyVd5RH9gyTt2+J/yUmYlIR0s04n6+7Vm1ozezUeLEaUjhaDSuXHwVRgvLJn1tQ7xiuVv/ocTRF42mNgZGBgYGbwZOBiAAFGJBIMAAizAFoAAABiAGIAznjaY2BkYGAA4in8zwXi+W2+MjCzMIDApSwvXzC97Z4Ig8N/BxYGZgcgl52BCSQKAA3jCV8CAABfAAAAAAQAAEB42mNgZGBg4f3vACQZQABIMjKgAmYAKEgBXgAAeNpjYGY6wTiBgZWBg2kmUxoDA4MPhGZMYzBi1AHygVLYQUCaawqDA4PChxhmh/8ODDEsvAwHgMKMIDnGL0x7gJQCAwMAJd4MFwAAAHjaY2BgYGaA4DAGRgYQkAHyGMF8NgYrIM3JIAGVYYDT+AEjAwuDFpBmA9KMDEwMCh9i/v8H8sH0/4dQc1iAmAkALaUKLgAAAHjaTY9LDsIgEIbtgqHUPpDi3gPoBVyRTmTddOmqTXThEXqrob2gQ1FjwpDvfwCBdmdXC5AVKFu3e5MfNFJ29KTQT48Ob9/lqYwOGZxeUelN2U2R6+cArgtCJpauW7UQBqnFkUsjAY/kOU1cP+DAgvxwn1chZDwUbd6CFimGXwzwF6tPbFIcjEl+vvmM/byA48e6tWrKArm4ZJlCbdsrxksL1AwWn/yBSJKpYbq8AXaaTb8AAHja28jAwOC00ZrBeQNDQOWO//sdBBgYGRiYWYAEELEwMTE4uzo5Zzo5b2BxdnFOcALxNjA6b2ByTswC8jYwg0VlNuoCTWAMqNzMzsoK1rEhNqByEyerg5PMJlYuVueETKcd/89uBpnpvIEVomeHLoMsAAe1Id4AAAAAAAB42oWQT07CQBTGv0JBhagk7HQzKxca2sJCE1hDt4QF+9JOS0nbaaYDCQfwCJ7Au3AHj+LO13FMmm6cl7785vven0kBjHCBhfpYuNa5Ph1c0e2Xu3jEvWG7UdPDLZ4N92nOm+EBXuAbHmIMSRMs+4aUEd4Nd3CHD8NdvOLTsA2GL8M9PODbcL+hD7C1xoaHeLJSEao0FEW14ckxC+TU8TxvsY6X0eLPmRhry2WVioLpkrbp84LLQPGI7c6sOiUzpWIWS5GzlSgUzzLBSikOPFTOXqly7rqx0Z1Q5BAIoZBSFihQYQOOBEdkCOgXTOHA07HAGjGWiIjaPZNW13/+lm6S9FT7rLHFJ6fQbkATOG1j2OFMucKJJsxIVfQORl+9Jyda6Sl1dUYhSCm1dyClfoeDve4qMYdLEbfqHf3O/AdDumsjAAB42mNgYoAAZQYjBmyAGYQZmdhL8zLdDEydARfoAqIAAAABAAMABwAKABMAB///AA8AAQAAAAAAAAAAAAAAAAABAAAAAA==) format('woff');
  }
  
  body {
    -webkit-text-size-adjust: 100%;
    text-size-adjust: 100%;
    color: #333;
    font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
    font-size: 16px;
    line-height: 1.6;
    word-wrap: break-word;
    width: 728px;
    max-width: 99%;
    box-sizing: border-box;
    padding: 30px 30px 8rem 30px;
    margin-left: auto;
    margin-right: auto;
  }
  
  body a {
    background-color: transparent;
  }
  
  body a:active,
  body a:hover {
    outline: 0;
  }
  
  body strong {
    font-weight: bold;
  }
  
  body h1 {
    font-size: 2em;
    margin: 0.67em 0;
  }
  
  body img {
    border: 0;
  }
  
  body hr {
    box-sizing: content-box;
    height: 0;
  }
  
  body pre {
    overflow: auto;
  }
  
  body code,
  body kbd,
  body pre {
    font-family: monospace, monospace;
    font-size: 1em;
  }
  
  body input {
    color: inherit;
    font: inherit;
    margin: 0;
  }
  
  body html input[disabled] {
    cursor: default;
  }
  
  body input {
    line-height: normal;
  }
  
  body input[type="checkbox"] {
    box-sizing: border-box;
    padding: 0;
  }
  
  body table {
    border-collapse: collapse;
    border-spacing: 0;
  }
  
  body td,
  body th {
    padding: 0;
  }
  
  body * {
    box-sizing: border-box;
  }
  
  body input {
    font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  }
  
  body a {
    color: #4078c0;
    text-decoration: none;
  }
  
  body a:hover,
  body a:active {
    text-decoration: underline;
  }
  
  body hr {
    height: 0;
    margin: 15px 0;
    overflow: hidden;
    background: transparent;
    border: 0;
    border-bottom: 1px solid #ddd;
  }
  
  body hr:before {
    display: table;
    content: "";
  }
  
  body hr:after {
    display: table;
    clear: both;
    content: "";
  }
  
  body h1,
  body h2,
  body h3,
  body h4,
  body h5,
  body h6 {
    margin-top: 15px;
    margin-bottom: 15px;
    line-height: 1.1;
  }
  
  body h1 {
    font-size: 30px;
  }
  
  body h2 {
    font-size: 21px;
  }
  
  body h3 {
    font-size: 16px;
  }
  
  body h4 {
    font-size: 14px;
  }
  
  body h5 {
    font-size: 12px;
  }
  
  body h6 {
    font-size: 11px;
  }
  
  body blockquote {
    margin: 0;
  }
  
  body ul,
  body ol {
    padding: 0;
    margin-top: 0;
    margin-bottom: 0;
  }
  
  body ol ol,
  body ul ol {
    list-style-type: lower-roman;
  }
  
  body ul ul ol,
  body ul ol ol,
  body ol ul ol,
  body ol ol ol {
    list-style-type: lower-alpha;
  }
  
  body dd {
    margin-left: 0;
  }
  
  body code {
    font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
    font-size: 12px;
  }
  
  body pre {
    margin-top: 0;
    margin-bottom: 0;
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  }
  
  body .select::-ms-expand {
    opacity: 0;
  }
  
  body .octicon {
    font: normal normal normal 16px/1 octicons-link;
    display: inline-block;
    text-decoration: none;
    text-rendering: auto;
    -webkit-font-smoothing: antialiased;
    -moz-osx-font-smoothing: grayscale;
    -webkit-user-select: none;
    -moz-user-select: none;
    -ms-user-select: none;
    user-select: none;
  }
  
  body .octicon-link:before {
    content: '\f05c';
  }
  
  body:before {
    display: table;
    content: "";
  }
  
  body:after {
    display: table;
    clear: both;
    content: "";
  }
  
  body>*:first-child {
    margin-top: 0 !important;
  }
  
  body>*:last-child {
    margin-bottom: 0 !important;
  }
  
  body a:not([href]) {
    color: inherit;
    text-decoration: none;
  }
  
  body .anchor {
    display: inline-block;
    padding-right: 2px;
    margin-left: -18px;
  }
  
  body .anchor:focus {
    outline: none;
  }
  
  body h1,
  body h2,
  body h3,
  body h4,
  body h5,
  body h6 {
    margin-top: 1em;
    margin-bottom: 16px;
    font-weight: bold;
    line-height: 1.4;
  }
  
  body h1 .octicon-link,
  body h2 .octicon-link,
  body h3 .octicon-link,
  body h4 .octicon-link,
  body h5 .octicon-link,
  body h6 .octicon-link {
    color: #000;
    vertical-align: middle;
    visibility: hidden;
  }
  
  body h1:hover .anchor,
  body h2:hover .anchor,
  body h3:hover .anchor,
  body h4:hover .anchor,
  body h5:hover .anchor,
  body h6:hover .anchor {
    text-decoration: none;
  }
  
  body h1:hover .anchor .octicon-link,
  body h2:hover .anchor .octicon-link,
  body h3:hover .anchor .octicon-link,
  body h4:hover .anchor .octicon-link,
  body h5:hover .anchor .octicon-link,
  body h6:hover .anchor .octicon-link {
    visibility: visible;
  }
  
  body h1 {
    padding-bottom: 0.3em;
    font-size: 1.75em;
    line-height: 1.2;
  }
  
  body h1 .anchor {
    line-height: 1;
  }
  
  body h2 {
    padding-bottom: 0.3em;
    font-size: 1.5em;
    line-height: 1.225;
  }
  
  body h2 .anchor {
    line-height: 1;
  }
  
  body h3 {
    font-size: 1.25em;
    line-height: 1.43;
  }
  
  body h3 .anchor {
    line-height: 1.2;
  }
  
  body h4 {
    font-size: 1em;
  }
  
  body h4 .anchor {
    line-height: 1.2;
  }
  
  body h5 {
    font-size: 1em;
  }
  
  body h5 .anchor {
    line-height: 1.1;
  }
  
  body h6 {
    font-size: 1em;
    color: #777;
  }
  
  body h6 .anchor {
    line-height: 1.1;
  }
  
  body p,
  body blockquote,
  body ul,
  body ol,
  body dl,
  body table,
  body pre {
    margin-top: 0;
    margin-bottom: 16px;
  }
  
  body hr {
    height: 4px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
  }
  
  body ul,
  body ol {
    padding-left: 2em;
  }
  
  body ul ul,
  body ul ol,
  body ol ol,
  body ol ul {
    margin-top: 0;
    margin-bottom: 0;
  }
  
  body li>p {
    margin-top: 16px;
  }
  
  body dl {
    padding: 0;
  }
  
  body dl dt {
    padding: 0;
    margin-top: 16px;
    font-size: 1em;
    font-style: italic;
    font-weight: bold;
  }
  
  body dl dd {
    padding: 0 16px;
    margin-bottom: 16px;
  }
  
  body blockquote {
    padding: 0 15px;
    color: #777;
    border-left: 4px solid #ddd;
  }
  
  body blockquote>:first-child {
    margin-top: 0;
  }
  
  body blockquote>:last-child {
    margin-bottom: 0;
  }
  
  body table {
    display: block;
    width: 100%;
    overflow: auto;
    word-break: normal;
    word-break: keep-all;
  }
  
  body table th {
    font-weight: bold;
  }
  
  body table th,
  body table td {
    padding: 6px 13px;
    border: 1px solid #ddd;
  }
  
  body table tr {
    background-color: #fff;
    border-top: 1px solid #ccc;
  }
  
  body table tr:nth-child(2n) {
    background-color: #f8f8f8;
  }
  
  body img {
    max-width: 100%;
    box-sizing: content-box;
    background-color: #fff;
  }
  
  body code {
    padding: 0;
    padding-top: 0;
    padding-bottom: 0;
    margin: 0;
    font-size: 85%;
    background-color: rgba(0,0,0,0.04);
    border-radius: 3px;
  }
  
  body code:before,
  body code:after {
    letter-spacing: -0.2em;
    content: "\00a0";
  }
  
  body pre>code {
    padding: 0;
    margin: 0;
    font-size: 100%;
    word-break: normal;
    white-space: pre;
    background: transparent;
    border: 0;
  }
  
  body .highlight {
    margin-bottom: 16px;
  }
  
  body .highlight pre,
  body pre {
    padding: 16px;
    overflow: auto;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border-radius: 3px;
  }
  
  body .highlight pre {
    margin-bottom: 0;
    word-break: normal;
  }
  
  body pre {
    word-wrap: normal;
  }
  
  body pre code {
    display: inline;
    max-width: initial;
    padding: 0;
    margin: 0;
    overflow: initial;
    line-height: inherit;
    word-wrap: normal;
    background-color: transparent;
    border: 0;
  }
  
  body pre code:before,
  body pre code:after {
    content: normal;
  }
  
  body kbd {
    display: inline-block;
    padding: 3px 5px;
    font-size: 11px;
    line-height: 10px;
    color: #555;
    vertical-align: middle;
    background-color: #fcfcfc;
    border: solid 1px #ccc;
    border-bottom-color: #bbb;
    border-radius: 3px;
    box-shadow: inset 0 -1px 0 #bbb;
  }
  
  body .pl-c {
    color: #969896;
  }
  
  body .pl-c1,
  body .pl-s .pl-v {
    color: #0086b3;
  }
  
  body .pl-e,
  body .pl-en {
    color: #795da3;
  }
  
  body .pl-s .pl-s1,
  body .pl-smi {
    color: #333;
  }
  
  body .pl-ent {
    color: #63a35c;
  }
  
  body .pl-k {
    color: #a71d5d;
  }
  
  body .pl-pds,
  body .pl-s,
  body .pl-s .pl-pse .pl-s1,
  body .pl-sr,
  body .pl-sr .pl-cce,
  body .pl-sr .pl-sra,
  body .pl-sr .pl-sre {
    color: #183691;
  }
  
  body .pl-v {
    color: #ed6a43;
  }
  
  body .pl-id {
    color: #b52a1d;
  }
  
  body .pl-ii {
    background-color: #b52a1d;
    color: #f8f8f8;
  }
  
  body .pl-sr .pl-cce {
    color: #63a35c;
    font-weight: bold;
  }
  
  body .pl-ml {
    color: #693a17;
  }
  
  body .pl-mh,
  body .pl-mh .pl-en,
  body .pl-ms {
    color: #1d3e81;
    font-weight: bold;
  }
  
  body .pl-mq {
    color: #008080;
  }
  
  body .pl-mi {
    color: #333;
    font-style: italic;
  }
  
  body .pl-mb {
    color: #333;
    font-weight: bold;
  }
  
  body .pl-md {
    background-color: #ffecec;
    color: #bd2c00;
  }
  
  body .pl-mi1 {
    background-color: #eaffea;
    color: #55a532;
  }
  
  body .pl-mdr {
    color: #795da3;
    font-weight: bold;
  }
  
  body .pl-mo {
    color: #1d3e81;
  }
  
  body kbd {
    display: inline-block;
    padding: 3px 5px;
    font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
    line-height: 10px;
    color: #555;
    vertical-align: middle;
    background-color: #fcfcfc;
    border: solid 1px #ccc;
    border-bottom-color: #bbb;
    border-radius: 3px;
    box-shadow: inset 0 -1px 0 #bbb;
  }
  
  body .task-list-item {
    list-style-type: none;
  }
  
  body .task-list-item+.task-list-item {
    margin-top: 3px;
  }
  
  body .task-list-item input {
    margin: 0 0.35em 0.25em -1.6em;
    vertical-align: middle;
  }
  
  body :checked+.radio-label {
    z-index: 1;
    position: relative;
    border-color: #4078c0;
  }
  </style>
</head>
<body>
<h1 id="machine-learning">Machine Learning</h1>
<h1 id="w1---intro">W1 - Intro</h1>
<h2 id="regression">Regression</h2>
<p><strong>Regression</strong> is relating input variables to output variables to <em>predict</em> (find output for an unknown input sampled from the same distribution as the input examples) or <em>interpret</em> / understand the effects of input on output.</p>
<p><strong>Data</strong> used for regression is <span class="math inline">\(N\)</span> pairs of <span class="math inline">\((x_n;y_n)\)</span> where <span class="math inline">\(x_n \in \mathbb{R}^D\)</span> .</p>
<p>A <strong>regression function</strong> approximates the output “well enough” for given inputs. Approximation is often in <em>Least-squares sense</em> :</p>
<p><span class="math display">\[\underset{f}{min} \frac{1}{N} \sum_{n=1}^N (f(x_n) - y_n)^2 \tag{1}\]</span></p>
<p>which should yield a <span class="math inline">\(f(x_n) \approx y_n \text{ , } \forall n\)</span>.</p>
<p>Note that regression does not find a <em>causal</em> relationship but a <em>correlation</em>.</p>
<h2 id="linear-regression">Linear Regression</h2>
<p>A <strong>model</strong> that assumes a linear relationship between input and output.</p>
<h4 id="simple-linear-regression-1-dimension">Simple linear regression (1 dimension)</h4>
<p><span class="math display">\[y_n \approx f(x_n) := w_0 + w_1 x_{n1} \tag{2}\]</span></p>
<p>where <span class="math inline">\(w=(w_0;w_1)\)</span> are the model parameters.</p>
<h4 id="multivariate-linear-regression-d-dimensions">Multivariate linear regression (D dimensions)</h4>
<p><span class="math display">\[y_n \approx f(x_n) := w_0 + w_1x_{n1} + \dots + w_Dx_{nD} = \tilde{x}_n^T w \tag{3}\]</span></p>
<p>where <span class="math inline">\(w = \begin{bmatrix} w_0 \\ w_1 \\ \dots \\ w_D \end{bmatrix}\)</span> and <span class="math inline">\(\tilde{x}_n = \begin{bmatrix} 1 \\ x_1 \\ \dots \\ x_{nD} \end{bmatrix} = \begin{bmatrix} 1 \\ x_n \end{bmatrix}\)</span></p>
<h4 id="learning">Learning</h4>
<p>Given data we want to find <span class="math inline">\(\mathbf{w} = \begin{bmatrix}w_0 \\ \dots \\ w_D \end{bmatrix}\)</span> which minimizes the error.</p>
<p><em>Note</em> <strong>the <span class="math inline">\(D &gt; N\)</span> problem</strong>. If the number of parameters is greater than the number of samples the problem is underdetermined (can be solved through <em>regularization</em>).</p>
<h2 id="cost-functions">Cost functions</h2>
<p>Quantifying how well our model is doing (or how costly our mistakes are).</p>
<p>Desirable properties :</p>
<ul>
<li><strong>symmetry</strong> : positive and negative errors should be penalized equally.</li>
<li><strong>robustness</strong> : error should be capped so that big errors / outliers don’t cause wild swings.</li>
</ul>
<p><strong>trade-offs</strong> : Statistical properties vs computational properties.</p>
<h4 id="mse">MSE</h4>
<p>Mean Squared Error</p>
<p><span class="math display">\[ MSE(\mathbf{w}) := \frac{1}{N} \sum_{n=1}^{N} [y_n - f(\mathbf{x_n})]^2 \tag{4}\]</span></p>
<h4 id="mae">MAE</h4>
<p>Mean Absolute Error</p>
<p><span class="math display">\[MAE(\mathbf{w}) := \frac{1}{N} \sum_{n=1}^{N} |y_n - f(\mathbf{x_n})| \tag{5}\]</span></p>
<h4 id="convexity">Convexity</h4>
<p>A function <span class="math inline">\(f(u)\)</span> is <strong>convex</strong> iff <span class="math inline">\(\forall u,v \in \chi\)</span> and <span class="math inline">\(\forall \lambda \in [0,1]\)</span> we have <span class="math inline">\(f(\lambda u + (1-\lambda) v) \leqslant \lambda f(u) + (1-\lambda) f(v)\)</span></p>
<p><em>Note</em> convexity is strict if true <span class="math inline">\(\forall \lambda \in \; ]0,1[\)</span></p>
<p><strong>A strictly convex function has a unique global minimum <span class="math inline">\(w^*\)</span></strong> (i.e. every local minimum is a global minimum).</p>
<p>Note that a sum of convex functions is convex (MSE is convex).</p>
<h4 id="other-cost-functions">Other cost functions</h4>
<p><strong>Hubber loss</strong> convex, differentiable, robust to outliers but hard to define <span class="math inline">\(\delta\)</span></p>
<p><span class="math display">\[ Hubber = \begin{cases} 0.5e^2 \; \text{ , if } |e| \leqslant \delta \\ \delta |e| - 0.5 \delta^2  \; \text{ , if } |e| &gt; \delta \end{cases} \]</span></p>
<p><strong>Tukey’s bisquare loss</strong> non-convex but robust to outliers</p>
<p><span class="math display">\[ \frac{\partial L}{\partial e} = \begin{cases} e [1 - \frac{e^2}{\delta^2}]^2 \text{ , if } |e| \leqslant \delta \\ 0 \text{ otherwise } \end{cases} \]</span></p>
<h2 id="w2---optimization">W2 - Optimization</h2>
<h3 id="learning-estimation-fitting">Learning / Estimation / Fitting</h3>
<p>Given a loss function <span class="math inline">\(\mathcal{L}(w)\)</span> we want to find <span class="math inline">\(\mathbf{w^*}\)</span> which minimizes the loss/cost :</p>
<p><span class="math display">\[\mathbf{w^*} = \underset{w}{min} \ \mathcal{L}(w) \text{ with } \mathbf{w} \in \!R^D \tag{6}\]</span></p>
<p>The <strong>learning</strong> problem is an <strong>optimization</strong> problem.</p>
<h3 id="gridsearch">Gridsearch</h3>
<p>Compute the cost over all values of <span class="math inline">\(\mathbf{w}\)</span> in a grid and pick the best value.</p>
<p>This is brute-force : inefficient (exponential computational complexity : <span class="math inline">\(n^D\)</span> possibilities where <span class="math inline">\(n\)</span> is the number of values in the grid, <span class="math inline">\(D\)</span> the dimensionality of <span class="math inline">\(\textbf{w}\)</span>) but will work anywhere. Note that there is no guarantee of a global optimum.</p>
<h3 id="optimization-landscapes">Optimization landscapes</h3>
<figure>
<img src="img/1.png" alt="1" /><figcaption>1</figcaption>
</figure>
<p>A vector <span class="math inline">\(\mathbf{w^*}\)</span> is a <em>local minimum</em> of <span class="math inline">\(\mathcal{L}\)</span> if it no worse than its neighbours :</p>
<p><span class="math display">\[ \mathcal{L}(\mathbf{w^*}) \le \mathcal{L}(\mathbf{w}), \text{ } \forall \mathbf{w} \text{ with } ||\mathbf{w} - \mathbf{w^*}|| &lt; \epsilon \text{ , } \epsilon &gt; 0 \tag{7}\]</span></p>
<p>A vector <span class="math inline">\(\mathbf{w^*}\)</span> is a <em>global minimum</em> of <span class="math inline">\(\mathcal{L}\)</span> if it no worse than all others :</p>
<p><span class="math display">\[ \mathcal{L}(\mathbf{w^*}) \le \mathcal{L}(\mathbf{w}), \text{ } \forall \mathbf{w} \in \!R^D \tag{8}\]</span></p>
<h3 id="smooth-optimization">Smooth optimization</h3>
<p><em>In the case of a differentiable <span class="math inline">\(\mathcal{L}\)</span></em></p>
<h4 id="follow-the-gradient">Follow the gradient</h4>
<p><em>gradient</em> = slope of the tangent to the function, pointing to the direction of largest increase of the function.</p>
<p><span class="math display">\[\nabla \mathcal{L}(\mathbf{w}) := \begin{bmatrix} \frac{\partial \mathcal{L}}{\partial w_1} &amp; \dots &amp; \frac{\partial \mathcal{L}}{\partial w_D} \end{bmatrix}^T \in \!R^D \tag{9}\]</span></p>
<h4 id="batch-gradient-descent">Batch Gradient Descent</h4>
<p>To minimize the function we iteratively take a step in the (opposite) direction of the gradient :</p>
<p><span class="math display">\[\mathbf{w}^{t+1} := \mathbf{w}^{t} - \gamma \nabla \mathcal{L}(\mathbf{w^{(t)}})  \tag{10}\]</span></p>
<p>where <span class="math inline">\(\gamma &gt; 0\)</span> is the <em>learning rate</em> (usually <span class="math inline">\(1 \over t\)</span>)</p>
<p><strong>example</strong> GD for MSE : <span class="math inline">\(w_0^{(t+1)} := (1-\gamma)w_0^{(t)} + \gamma \bar{y}\)</span> where <span class="math inline">\(\bar{y} = \sum \frac{y_n}{N}\)</span>. Note that this only converges if <span class="math inline">\(\gamma \in ]0;2[\)</span>.</p>
<h3 id="gradient-descent-for-linear-mse">Gradient Descent for Linear MSE</h3>
<p>We define the error vector : <span class="math inline">\(e = y - Xw\)</span> and the MSE : <span class="math inline">\(\mathcal{L}(w) := \frac{1}{2N}e^Te\)</span>. The gradient is then : <span class="math inline">\(\nabla \mathcal{L} = - \frac{1}{N} X^Te\)</span>.</p>
<p>Complexity : <code>O(N.D)</code></p>
<h3 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h3>
<p>We can rewrite the loss as the sum of losses over training examples :</p>
<p><span class="math inline">\(\mathcal{L}(w) = \frac{1}{N} \sum{\mathcal{L}_n(w^{(t)})} \tag{11}\)</span></p>
<p>The SGD algorithm uses the same update rule as GD, but chosing one of the gradients from the training set, at random :</p>
<p><span class="math display">\[\mathbf{w}^{t+1} := \mathbf{w}^{t} - \gamma \nabla \mathcal{L_n}(\mathbf{w^{(t)}})  \tag{12}\]</span></p>
<p>The idea is to have a cheap but <em>unbiased</em> estimate of the gradient (the expectation for <span class="math inline">\(\nabla \mathcal{L_n}\)</span> over a random choice of n is <span class="math inline">\(\nabla\mathcal{L}\)</span>). The computational complexity is now <code>O(D)</code>.</p>
<p>There is also an intermediate version, called <em>mini-batch</em>, where the update direction being computed as the mean of a small set of gradients, chosen at random. This kind of computation can easily be <em>parallelized</em>.</p>
<h3 id="non-smooth-optimization">Non-smooth Optimization</h3>
<p><em>In the case of a non-differentiable loss function <span class="math inline">\(\mathcal{L}\)</span></em>.</p>
<h4 id="subgradient">Subgradient</h4>
<p><span class="math inline">\(\mathbf{g} \in \!R^D\)</span> , a sub gradient, is a vector such that : <span class="math inline">\(\mathcal{L}(u) \ge \mathcal{L}(w) + g^T(u-w) \ \ \forall u\)</span>. If <span class="math inline">\(\mathcal{L}\)</span> is differentiable at <span class="math inline">\(w\)</span>, then the only subgradient at <span class="math inline">\(w\)</span> is <span class="math inline">\(g = \nabla \mathcal{L}(w)\)</span>.</p>
<p>We can use this subgradient for a version of GD (and SGD), but using <span class="math inline">\(g\)</span> in place for <span class="math inline">\(\nabla \mathcal{L}\)</span>.</p>
<h3 id="constrained-optimization">Constrained optimization</h3>
<p>Sometimes the weight space is restricted : <span class="math inline">\(\mathbf{w} \in \mathcal{C} \subseteq \!R^D\)</span> where <span class="math inline">\(\mathcal{C}\)</span> is the constraint set.</p>
<h3 id="convex-sets">Convex sets</h3>
<p><span class="math inline">\(\mathcal{C}\)</span> is convex iff the lign segment between any two points of <span class="math inline">\(\mathcal{C}\)</span> lies in <span class="math inline">\(\mathcal{C}\)</span> : <span class="math inline">\(\theta u + (1-\theta) v \in \mathcal{C}\)</span></p>
<figure>
<img src="img/2.png" alt="2" /><figcaption>2</figcaption>
</figure>
<p><em>Note</em> Intersections of convex sets are convex. <em>Projections</em> onto convex sets are <em>unique</em>.</p>
<h3 id="projected-gradient-descent">Projected Gradient Descent</h3>
<p>A projection is formally defined as : <span class="math inline">\(P_{\mathcal{C}}(w^{&#39;}) := \underset{v\in \mathcal{C}}{arg min} ||v-w^{&#39;}||\)</span>. The update rule then becomes : <span class="math inline">\(w^{(t+1)} := P_{\mathcal{C}} [w^{(t)} - \gamma \nabla \mathcal{L}(w^{(t)})]\)</span>. This means we add a projection onto the constraint set after each step. We keep the same convergence properties.</p>
<h3 id="turning-constrained-into-unconstrained-problems">Turning Constrained into Unconstrained problems</h3>
<p>Use penalties for solutions inside the constraint space :</p>
<ul>
<li><em>brick wall</em> <span class="math inline">\(I_C(w) := \begin{cases} 0 \text{ if  } w \in C \\ \inf \text{ if } w \in C \end{cases} \Rightarrow \underset{w\in\!R^D}{min} \mathcal{L}(w) + \!I_C(w)\)</span></li>
<li><em>Penalize error</em> (linear constraint : <span class="math inline">\(C =\{ w \in \!R^D | Aw=b \}\)</span>) <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\underset{w\in\!R^D}{min} \ \mathcal{L}(w) + \lambda ||Aw -b ||^2\)</span></li>
</ul>
<h3 id="implementation-issues">Implementation issues</h3>
<ul>
<li>Stopping criterion : if gradient is ~ 0 we can stop</li>
<li>Optimality : if the <span class="math inline">\(\partial ^2\)</span> is <span class="math inline">\(&gt; 0\)</span> then it is a minimum. If the function is convex, only then can we know that it is a global optimum.</li>
<li>Step-size selection : if <span class="math inline">\(\gamma\)</span> is too large, the method might diverge (conversely convergence is slow if <span class="math inline">\(\gamma\)</span> is too small).</li>
<li>Line-search methods : For some objectives <span class="math inline">\(\mathcal{L}\)</span>, we can set step-size automatically using a line-search method.</li>
<li>Feature normalization : GD is very sensitive to ill-conditioning =&gt; advised to normalise input features.</li>
</ul>
<h2 id="w3--">W3 -</h2>
<h3 id="least-squares">Least Squares</h3>
<p>For linear regression with MSE cost fuction we can compute the optimum analytically (<em>note</em> this is rare) with a set of <em>normal equations</em>. This is called the <em>least-squares</em> solution.</p>
<p>We use the fact that for convex functions : <span class="math inline">\(\nabla \mathcal{L}(w^*) = 0\)</span>, which is a linear system of <span class="math inline">\(D\)</span> equations.</p>
<h4 id="normal-equations">Normal equations</h4>
<p>Linear regression with MSE is expressed as :</p>
<p><span class="math display">\[ \mathcal{L}(w) = \frac{1}{2N}\sum(y_n - x_n^T w)^2 = \frac{1}{2N} (\mathbf{y}-\mathbf{X}\mathbf{w})^T(\mathbf{y}-\mathbf{X}\mathbf{w}) \]</span></p>
<p>This cost function is <strong>convex</strong> in <span class="math inline">\(\mathbf{w}\)</span>. Proof methods :</p>
<ol type="1">
<li>Compute the Hessian (here <span class="math inline">\(\frac{1}{N}\mathbf{X}^T\mathbf{X}\)</span>) and show it is positive semi-definite (all eigenvalues are non-negative)</li>
<li>Show langrangian inequality</li>
<li>The loss is the linear composition of convex functions</li>
</ol>
<p>We compute the gradient : <span class="math inline">\(\nabla \mathcal{L}(\mathbf{w}) = -\frac{1}{N}\mathbf{X}^T(\mathbf{y}-\mathbf{X}\mathbf{w}) = 0\)</span></p>
<h4 id="geometric-interpretation">Geometric interpretation</h4>
<p>Let <span class="math inline">\(S\)</span> denote the space spanned by the columns of <span class="math inline">\(X\)</span>. Note that <span class="math inline">\(x = Xw\)</span> is an element of <span class="math inline">\(S\)</span>. I.e., by choosing <span class="math inline">\(w\)</span> we choose <span class="math inline">\(x \in S\)</span>. What element of <span class="math inline">\(S\)</span> shall we take? The normal equations tell us that the optimum choice for <span class="math inline">\(x\)</span>, call it <span class="math inline">\(x^*\)</span>, is that element so that <span class="math inline">\(y - x^*\)</span> is orthogonal to <span class="math inline">\(S\)</span>. In other words, we should pick <span class="math inline">\(x^*\)</span> to be equal to the projection of <span class="math inline">\(y\)</span> onto <span class="math inline">\(S\)</span>.</p>
<p>This means the normal equations can be rewritten as :</p>
<p><span class="math display">\[ \mathbf{X}^T\mathbf{X}\mathbf{w}^* = \mathbf{X}^T\mathbf{y} \tag{13}\]</span></p>
<p>The matrix <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is called the <em>Gram Matrix</em>. If it is invertible (i.e. it has full column rank <span class="math inline">\(rank(\mathbf{X}) = D\)</span>), we can compute a closed form expression for the minimum :</p>
<p><span class="math display">\[  \mathbf{w^*} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \tag{14}\]</span></p>
<p>We can use this to predict a new value for an unseen datapoint <span class="math inline">\(\mathbf{x_m}\)</span> : <span class="math inline">\(\hat{\mathbf{y}}_m := \mathbf{x}_m^T \mathbf{w^*}\)</span>.</p>
<h4 id="rank-deficiency">Rank deficiency</h4>
<p>In practice we often have a rank deficient <span class="math inline">\(\mathbf{X}\)</span> :</p>
<ul>
<li>if <span class="math inline">\(D &gt; N\)</span> we always have <span class="math inline">\(rank(\mathbf{X}) &lt; D\)</span></li>
<li>if <span class="math inline">\(D \le N\)</span> but some columns are (nearly) collinear then the matrix is ill-conditioned</li>
</ul>
<p>This means that there is often no unique way of representing the projection of <span class="math inline">\(\mathbf{w}\)</span> onto <span class="math inline">\(\mathcal{S}\)</span>. To get around this we use <strong>SVD</strong>.</p>
<h3 id="maximum-likelihood">Maximum Likelihood</h3>
<p>Instead of chosing a cost function and then, given data, finding a model that minimizes the cost function, ML start in a probabilistic way to fit the data.</p>
<h4 id="probabilistic-model">probabilistic model</h4>
<p>We assume the data was generated by the model : <span class="math inline">\(y_n = \mathbf{x}_n^T \mathbf{w} + \epsilon_n\)</span> where <span class="math inline">\(\epsilon_n\)</span> is a random, independent, zero-mean Gaussian noise with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The likelihood of a data vector <span class="math inline">\(\mathbf{y} = (y_1,\dots,y_N)\)</span> given the input <span class="math inline">\(\mathbf{X}\)</span> is :</p>
<p><span class="math display">\[ p(\mathbf{y}|\mathbf{X},\mathbf{w}) = \prod^N_{n=1}p(y_n|\mathbf{x}_n,\mathbf{w}) = \prod^N_{n=1}\mathcal{N}(y_n|\mathbf{x}_n^T\mathbf{w}, \sigma^2)  \tag{15}\]</span></p>
<p>The best model is then the one that maximises <span class="math inline">\((15)\)</span>.</p>
<p>The cost is modelled as the <em>log-likelihood</em> which gives :</p>
<p><span class="math display">\[ \mathcal{L}_{LL}(\mathbf{w}) := log \ p(y | X, w) = - \frac{1}{2\sigma^2} \sum(y_n-x_n^Tw)^2 + cnst\]</span></p>
<p>This is basically MSE with an added constant and <span class="math inline">\(N = -\sigma^2\)</span> : <span class="math inline">\(arg \ \underset{\mathbf{w}}{min}\ \mathcal{L}_{MSE}(\mathbf{w}) = arg \ \underset{\mathbf{w}}{max} \ \mathcal{L}_{MLE}(\mathbf{w}) \tag{16}\)</span></p>
<h3 id="underfitting-and-overfitting">Underfitting and Overfitting</h3>
<ul>
<li><em>underfit</em> : cannot find a function that is a good fit for the data</li>
<li><em>overfit</em> : the function we found fits noise as well</li>
</ul>
<h4 id="linear-models-and-underfitting">Linear models and underfitting</h4>
<p>If our model contains only linear functions of the scalar input we cannot match the function accurately, regardless of the number of samples and how small the noise is.</p>
<h4 id="linear-models-and-overfitting">Linear models and overfitting</h4>
<p>If we use a polynomial basis of a degree too high, the mdeol will fit the noise too closely.</p>
<p><strong>Note</strong> <em>Extended / Augmented Feature Vectors</em></p>
<p>We can augment the input to make linear models more powerful by defining a polynomial basis : <span class="math inline">\(\phi(x_n) = [1,x_n,x_n^2,\dots,x_n^M]\)</span>. We then fit the linear model to the extended feature vector : <span class="math inline">\(y_n \approx w_o + w_1x_n + w_2x_n^2 + \dots + w_Mx_n^M = \phi(x_n)^T\mathbf{w}\)</span>.</p>
<h3 id="regularization-ridge-and-lasso">Regularization : Ridge and Lasso</h3>
<p>Regularization penalises complex models and favor simpler models : <span class="math inline">\(\underset{W}{min} \ \mathcal{L}(w) + \Omega(w)\)</span>.</p>
<h4 id="l_2-regularization-ridge-regression"><span class="math inline">\(L_2\)</span>-Regularization : Ridge Regression</h4>
<p>This is the most commonly used regularizer : <span class="math inline">\(\Omega(w) = \lambda ||w||_2^2 = \lambda \sum_i w_i^2\)</span>. It penalizes large weights. When the loss function is MSE, this is called ridge regression, where the least-squares solutions is a special case for <span class="math inline">\(\lambda = 0\)</span>.</p>
<p>Explicit solution for <span class="math inline">\(w^*\)</span> : <span class="math inline">\(\mathbf{w_{ridge}^*} = (X^TX+\lambda^{&#39;}I)^{-1} X^Ty\)</span></p>
<p><em>Note</em> Since the eigenvalues of <span class="math inline">\(X^TX+\lambda^{&#39;}I\)</span> are always at least <span class="math inline">\(\lambda^{&#39;}\)</span> (<em>eigenvalue lifting</em>), which prevents ill-conditioning (the inverse always exists so a closed-form solution can be found).</p>
<h4 id="l_1-regularization-the-lasso"><span class="math inline">\(L_1\)</span>-Regularization : The Lasso</h4>
<p>In this case we use the <span class="math inline">\(L_1\)</span> norm : <span class="math inline">\(\Omega(w) = \lambda ||\mathbf{w}||_1 = \lambda \sum_i |w_i|\)</span>.</p>
<p>In combination with the MSE function this is known as the <strong>Lasso</strong>. It yields a sparse solution compared to <span class="math inline">\(L_2\)</span>-norm.</p>
<p>[TODO : review, still not clear]</p>
<h2 id="w4--">W4 -</h2>
<h3 id="model-selection">Model Selection</h3>
<p>Choice of the hyperparameters (<span class="math inline">\(\lambda\)</span> for regularization, <span class="math inline">\(d\)</span> for the degree of a polynomial fitting, …).</p>
<p>Let ’s assume that the dataset is a set of independant samples from a distribution <span class="math inline">\(\mathcal{D}\)</span> (<span class="math inline">\(S = \{(\mathbf{x_n},y_n) \ iid \ \sim \mathcal{D} \}^N_{n=1}\)</span>), from which we lean a prediction function <span class="math inline">\(f_S\)</span>.</p>
<p>Since we do not have access to <span class="math inline">\(\mathcal{D}\)</span> directly, we should compute the empirical quantity (approximating the expected error over all samples chosen in <span class="math inline">\(\mathcal{D}\)</span>) : <span class="math display">\[L_S(f) = \frac{1}{|S|} \underset{(\mathbf{x_n}, y_n) \in S}{\sum} l(y_n, f(x_n))\]</span> but since the function <span class="math inline">\(f\)</span> is computed from data, this is actually just the training error.</p>
<h4 id="traintest-split">Train/Test split</h4>
<p>Split the data into two sets <span class="math inline">\(S_t\)</span> for training and <span class="math inline">\(S_v\)</span> for validation. We use use <span class="math inline">\(S_t\)</span> to compute the function <span class="math inline">\(f_{S_t}\)</span> and get an error for the training data by trying to predict the validation set :</p>
<p><span class="math display">\[L_{S_v}(f_{S_t}) = \frac{1}{|S_v|} \underset{(\mathbf{x_n}, y_n) \in S_v}{\sum} l(y_n, f_{S_t}(\mathbf{x_n})) \tag{17}\]</span></p>
<p>Since <span class="math inline">\(S_v\)</span> is a fresh set of sample we hope that <span class="math inline">\(L_{S_v}(f_{S_t})\)</span> is close to <span class="math inline">\(L_{\mathcal{D}}(f_{S_t})\)</span> (i.e. that they have the same expectation value).</p>
<p>To select the model we then run the learning on several values of the hyperparameters to find the functions <span class="math inline">\(f_{S_t,\lambda_k}\)</span> and compute the respective errors <span class="math inline">\(L_{S_v}(f_{S_t, \lambda_k})\)</span>, chosing <span class="math inline">\(\lambda_k\)</span> for which the error is smallest.</p>
<p><em>Problems</em></p>
<ul>
<li>How do we know that <span class="math inline">\(f_{S_t,\lambda_k}\)</span> is a good approximation of <span class="math inline">\(f_{\mathcal{D}}\)</span> ? (How do we know if this is the best model we can do ?)</li>
<li>How do we know that <span class="math inline">\(L_{S_v}(f)\)</span> is close to <span class="math inline">\(L_{\mathcal{D}}(f)\)</span> ? (How do we know that empirical error is close to the true error ?)</li>
</ul>
<h4 id="true-error-empirical-error">True error / empirical error</h4>
<p><span class="math display">\[ \mathbb{P} \{ max_k|L_{\mathcal{D}}()f_k - L_{S_v}(f_k)| \ge \sqrt{\frac{(b-a)^2ln(\frac{2K}{ \delta})}{2|S_v|}} \} \le \delta \tag{18}\]</span></p>
<ul>
<li>the error goes down at least like one over the square root of the number of validation points. The more data points we have therefore, the more condent we can be that the empirical loss we measure is close to the true loss.</li>
<li>Further, if we test K hyperparameters our error only goes up by a very small factor which is proportional to <span class="math inline">\(\sqrt{ ln(K)}\)</span>. So we can test quite a few different models without incurring a large penalty.</li>
</ul>
<p>Proof : Chernoff Bound Lemma</p>
<p>Let <span class="math inline">\(\Theta_1, \dots, \Theta_N\)</span> be a sequence of iid random variable with mean <span class="math inline">\(\mathbb{E}[\Theta]\)</span> and range <span class="math inline">\([a,b]\)</span>. Then for any <span class="math inline">\(\epsilon &gt; 0\)</span> :</p>
<p><span class="math display">\[ \mathbb{P} \{ |\frac{1}{N} \sum_{n=1}^N \Theta_n - \mathbb{E}[\Theta] | \ge \epsilon \} \le 2e^{-\frac{2N\epsilon^2}{(b-a)^2}} \]</span></p>
<h4 id="cross-validation">Cross-validation</h4>
<p><em>K-fold</em> : Randomly partition into <span class="math inline">\(K\)</span> groups, then train train <span class="math inline">\(K\)</span> times. Each time, leave out one of the <span class="math inline">\(K\)</span> groups for testing (using the other <span class="math inline">\(K-1\)</span> groups for training). Average the <span class="math inline">\(K\)</span> results and chose the best hyperparams over all runs.</p>
<p>Cross-validation returns an unbiased estimate of the <em>generalization error</em> and its variance.</p>
<h3 id="bias-variance-decomposition">Bias-Variance Decomposition</h3>
<p>Consider a regression problem with a 1D input and a polynomial basis as features. In this case the maximum degree we allow, call it <span class="math inline">\(d\)</span>, regulates the complexity of the class (<em>the same principle applies whenever we have a parameter that regulates the complexity of the model (e.g., in the ridge regression problem this is the parameter))</em>.</p>
<p>When <span class="math inline">\(d\)</span> is small then it is likely that we cannot find a good prediction function within our model to fit the data. We are saying that we are having a <strong>large bias</strong> (badfit, underfit). But since we are only considering very simple models (e.g., a constant, or ane function) even with relatively little data <span class="math inline">\(S\)</span>, we are likely to always learn essentially the same function <span class="math inline">\(f_S\)</span> and so the associated loss will vary little as a function of <span class="math inline">\(S\)</span>. Hence the <strong>variance</strong> of <span class="math inline">\(L_{D}(f_S)\)</span> (the variations due to the random sample <span class="math inline">\(S\)</span>) is <strong>small</strong>. In summary: <strong>for simple models we expect a large bias but a small variance</strong>.</p>
<p>When <span class="math inline">\(d\)</span> is very large, there are likely functions contained in the model that result in a very good fit with the given data and so we expect a <strong>small bias</strong>. But we are in danger of overtting the data. Even changing a single sample from <span class="math inline">\(S_t\)</span> might change the resulting prediction function <span class="math inline">\(f_S\)</span> considerably. We therefore expect to see a high variance of <span class="math inline">\(L_D(f_S)\)</span> as a function of <span class="math inline">\(S\)</span>. In summary: <strong>for complex models we expect small bias but high variances.</strong></p>
<h4 id="error-decomposition">Error decomposition</h4>
<p>Let <span class="math inline">\(e = f(\mathbf{x}_0 + \epsilon - f_{S_t}(\mathbf{x}_0))\)</span> be the error for this specific data generation model (<span class="math inline">\(y=f(\mathbf{x}) + \epsilon\)</span>)</p>
<p><span class="math display">\[ \mathbb{E}[e^2] = Var_{\epsilon \sim \mathcal{D}_{\epsilon}}[\epsilon] + (f(\mathbf{x_0}) - \mathbb{E}[f_{S^{&#39;}_t(\mathbf{x_0})}])^2 + \mathbb{E}[\mathbb{E}[ f_{S^{&#39;}_t}  (\mathbf{x_0})] - f_{S_t}(\mathbf{x}_0)^2] \\ = \text{noise variance + bias + variance}\]</span></p>
</body>
</html>
